{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 2010 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictors ##\n",
    "trips_data = pd.read_csv(\"Raw_Data/Trips_by_Distance.csv\")\n",
    "covid_19_data = pd.read_csv(\"Raw_Data/all-states-history-correct-range.csv\")\n",
    "\n",
    "## Response ##\n",
    "# Starts Apr 23, Ends Nov 9\n",
    "anxiety_depression_data = pd.read_csv(\"Raw_Data/Indicators_of_Anxiety_or_Depression_Based_on_Reported_Frequency_of_Symptoms_During_Last_7_Days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reduce dimensionality from county -> state \n",
    "td = trips_data.groupby(by = ['Date', 'State Postal Code']).sum().drop(columns= ['State FIPS', 'County FIPS']).reset_index()\n",
    "td['Period'] = np.floor(td.index / 51 / 7)\n",
    "# Create Period column\n",
    "td2 = td.groupby(by = ['Period', 'State Postal Code']).sum().reset_index()\n",
    "td2[\"Period\"] = td2[\"Period\"] - 1\n",
    "td2.loc[td2[\"Period\"] == -1,\"Period\"] = 0\n",
    "# Index 1-11 are 1 week\n",
    "# Index 12-16 are break\n",
    "for i in range(12, 17):\n",
    "    td2.loc[td2[\"Period\"] == i,\"Period\"] = 12\n",
    "\n",
    "# Index 17|18, 19|20, 21|22, 23|24, 25|26, 27|28 are one time period\n",
    "period = 13\n",
    "for i in range(0, 12, 2):\n",
    "    week = i + 17\n",
    "    td2.loc[td2[\"Period\"] == week,\"Period\"] = period\n",
    "    td2.loc[td2[\"Period\"] == week + 1,\"Period\"] = period\n",
    "    period += 1\n",
    "\n",
    "td2 = td2.groupby(by= [\"Period\", \"State Postal Code\"]).sum().reset_index()\n",
    "td2 = td2[td2['Period'] != 12.0]\n",
    "\n",
    "td2['Period'] = td2.apply(lambda x:\n",
    "                        x['Period']+1\n",
    "                        if x['Period'] < 12 \n",
    "                        else\n",
    "                        x['Period'], axis=1)\n",
    "\n",
    "trip_df = td2.copy()\n",
    "\n",
    "trip_df['State'] = trip_df['State Postal Code']\n",
    "trip_df = trip_df.drop('State Postal Code', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean COVID-19 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare covid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take desired columns from the data\n",
    "valuable_cols = ['state', 'date', 'death', 'deathConfirmed', \n",
    "                 'hospitalizedCurrently', 'positiveCasesViral', \n",
    "                 'positiveIncrease', 'totalTestsPeopleViral', \n",
    "                 'totalTestsPeopleViralIncrease', 'totalTestsViral', 'positiveTestsViral']\n",
    "covid_data = covid_19_data.copy()[valuable_cols]\n",
    "\n",
    "# Convert string to datetime\n",
    "covid_data['date'] = covid_data['date'].astype('datetime64[ns]')\n",
    "\n",
    "# 50 States + 1 Federal District (DC: District of Columbia)\n",
    "remove_states = ['AS', 'PR', 'GU', 'MP', 'VI']\n",
    "# AS: American Samoa\n",
    "# PR: Puerto Rico\n",
    "# GU: Guam\n",
    "# MP: Northern Mariana Islands\n",
    "# VI: US Virgin Islands\n",
    "states = list(set(covid_data['state'].values))\n",
    "final_states = [item for item in states if item not in remove_states]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Time Period column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These ranges match the time periods within our response data.\n",
    "ranges_list = ['4/23/2020-5/5/2020', '5/7/2020-5/12/2020', '5/14/2020-5/19/2020', '5/21/2020-5/26/2020',\n",
    "               '5/28/2020-6/2/2020', '6/4/2020-6/9/2020', '6/11/2020-6/16/2020', '6/18/2020-6/23/2020',\n",
    "               '6/25/2020-6/30/2020', '7/2/2020-7/7/2020', '7/9/2020-7/14/2020', '7/16/2020-7/21/2020',                  \n",
    "               '8/19/2020-8/31/2020', '9/2/2020-9/14/2020', '9/16/2020-9/28/2020', '9/30/2020-10/12/2020', \n",
    "               '10/14/2020-10/26/2020', '10/28/2020-11/9/2020']\n",
    "\n",
    "# Convert ranges_list to a list of lists of start and end datetimes\n",
    "period_list = []\n",
    "for r in ranges_list: # '4/23/2020-5/5/2020'\n",
    "    r_list = []\n",
    "    for date_str in r.split('-'): # [4/23/2020, 5/5/2020]\n",
    "        date = dt.datetime.strptime(date_str, '%m/%d/%Y').date() # 4/23/2020 -> datetime\n",
    "        r_list.append(date) # [start,end]\n",
    "    period_list.append(r_list) # [[start,end],[start,end]..]\n",
    "\n",
    "# Create Time Period column\n",
    "time_periods = [] \n",
    "    # 1-n: respective period by number \n",
    "    # -1: occur before first period\n",
    "    # -2: occur after last period\n",
    "    # 0: within period but not included\n",
    "for index, row in covid_data.iterrows():\n",
    "    true_period = np.NaN\n",
    "    if row['date'] < period_list[0][0]:\n",
    "        true_period = -1 # if occur before first period\n",
    "    elif row['date'] > period_list[-1][-1]:\n",
    "        true_period = -2 # if occur after last period\n",
    "    else: # else occur within a period\n",
    "        for period in period_list:\n",
    "            if period[0] <= row['date'] <= period[1]:\n",
    "                true_period = period_list.index(period) + 1\n",
    "    time_periods.append(true_period)\n",
    "# Add Time Period column to covid_data\n",
    "covid_data['Period'] = time_periods\n",
    "\n",
    "# Make a copy before cleaning\n",
    "data = covid_data.copy()\n",
    "\n",
    "# Remove rows with invalid time periods\n",
    "data.drop(data.loc[data['Period'] == -1].index, inplace=True)\n",
    "data.drop(data.loc[data['Period'] == -2].index, inplace=True)\n",
    "data.drop(data.loc[data['Period'] == np.NaN].index, inplace=True)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaN and groupby Time Period for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# empty list to fill with each state's cleaned data\n",
    "clean_data = []\n",
    "\n",
    "# set death equal to max from death and deathConfirmed\n",
    "data[\"death\"] = data[[\"death\", \"deathConfirmed\"]].max(axis=1)\n",
    "# set positiveCasesViral equal to max from positiveCasesViral,totalTestsViral, and totalTestsPeopleViral\n",
    "data[\"positiveCasesViral\"] = data[[\"positiveCasesViral\", \"totalTestsViral\", \"totalTestsPeopleViral\"]].max(axis=1)\n",
    "# drop redundant columns\n",
    "clean_df = data.drop(columns=[\"deathConfirmed\", \"totalTestsViral\", \"totalTestsPeopleViral\"], axis=1)\n",
    "\n",
    "for state in final_states:\n",
    "    state_data = clean_df.loc[clean_df['state'] == state].reset_index(drop=True)\n",
    "    # Convert cumulative columns -> increase-by columns\n",
    "    # by calculating difference of the day prior from each day\n",
    "    state_data['deathIncrease'] = state_data['death'].diff(+1)\n",
    "    state_data['positiveTestsViralIncrease'] = state_data['positiveTestsViral'].diff(+1)\n",
    "    state_data['positiveCasesIncrease'] = state_data['positiveCasesViral'].diff(+1)\n",
    "    # Positivity Rate interaction term\n",
    "    state_data[\"positivityRate\"] = state_data[\"positiveTestsViralIncrease\"] / state_data[\"totalTestsPeopleViralIncrease\"]\n",
    "    # drop redundant columns\n",
    "    state_data = state_data.drop(columns=['totalTestsPeopleViralIncrease'], axis=1)\n",
    "    # replace inf with 1\n",
    "    state_data = state_data.replace(math.inf, 1)\n",
    "    state_data = state_data.replace(-math.inf, 1)\n",
    "    # replace null positivity rates with 0\n",
    "    state_data[\"positivityRate\"] = state_data[\"positivityRate\"].replace(np.NaN, 0)\n",
    "    \n",
    "    # reduce dimensionality from days to periods\n",
    "    state_data = state_data.groupby(['Period', 'state']).mean().reset_index()\n",
    "    clean_data.append(state_data)\n",
    "\n",
    "# combine states to final df\n",
    "clean_df = pd.concat(clean_data)\n",
    "# drop rows with null values\n",
    "ignore_columns = ['Period', 'state','positiveTestsViral', 'positiveTestsViralIncrease']\n",
    "drop_by_columns = [item for item in list(clean_df.columns) if item not in ignore_columns]\n",
    "clean_df = clean_df.dropna(subset=drop_by_columns)\n",
    "covid_df = clean_df.copy()\n",
    "covid_df['State'] = covid_df['state']\n",
    "covid_df = covid_df.drop('state', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.to_csv(\"Clean_Data/clean_covid_data.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Anxiety/Depression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = anxiety_depression_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove demographic rows and uneeded columns\n",
    "clean_data = data.loc[data['State'] != 'United States']\n",
    "clean_data = clean_data.drop(['Phase', 'Group', 'Subgroup', 'Time Period Label',\n",
    "                             'Low CI', 'High CI', 'Confidence Interval', 'Quartile range'], axis=1)\n",
    "\n",
    "# Break each target into a unique dataframe\n",
    "depression_data = clean_data.loc[clean_data['Indicator'] == 'Symptoms of Depressive Disorder']\n",
    "anxiety_data = clean_data.loc[clean_data['Indicator'] == 'Symptoms of Anxiety Disorder']\n",
    "both_data = clean_data.loc[clean_data['Indicator'] == 'Symptoms of Anxiety Disorder or Depressive Disorder']\n",
    "\n",
    "# Merge each target back into one dataframe on State and Time Period\n",
    "# Each target now is displayed in a column, and the height of the column is divided by 3\n",
    "merged_data = pd.merge(depression_data, anxiety_data, on=['State', 'Time Period'])\n",
    "merged_data = pd.merge(merged_data, both_data, on=['State', 'Time Period'])\n",
    "\n",
    "# Clean final dataframe\n",
    "merged_data = merged_data.drop(['Indicator_x', 'Indicator_y', 'Indicator'], axis=1)\n",
    "merged_data.columns = ['State', 'Period', 'Depression_Score', 'Anxiety_Score', 'Mix_Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of states, used to change name -> acronym\n",
    "states_hash = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Federated States Of Micronesia': 'FM',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Marshall Islands': 'MH',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': ' ',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands': 'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Palau': 'PW',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change state name into acronym\n",
    "merged_data['State'] = merged_data.apply(lambda x: states_hash[x['State']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "response_df = merged_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df.to_csv(\"Clean_Data/clean_label_data.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge predictor and response datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = response_df + covid_df + trip_df\n",
    "final_df = pd.merge(response_df, covid_df, on=['Period', 'State'])\n",
    "final_df = pd.merge(final_df, trip_df, on=['Period', 'State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired column order 'positivityRate', 'positiveCasesIncrease', 'positiveTestsViralIncrease', \n",
    "columns = ['State', 'Period', 'Mix_Score', 'Depression_Score', 'Anxiety_Score',\n",
    "           'deathIncrease', 'death', 'hospitalizedCurrently', \n",
    "           'positivityRate',\n",
    "           'positiveTestsViralIncrease', 'positiveTestsViral',\n",
    "           'positiveCasesIncrease', 'positiveCasesViral',\n",
    "           'Population Staying at Home', 'Population Not Staying at Home',\n",
    "           'Number of Trips', 'Number of Trips <1', 'Number of Trips 1-3',\n",
    "           'Number of Trips 3-5', 'Number of Trips 5-10', 'Number of Trips 10-25',\n",
    "           'Number of Trips 25-50', 'Number of Trips 50-100',\n",
    "           'Number of Trips 100-250', 'Number of Trips 250-500',\n",
    "           'Number of Trips >=500']\n",
    "final_df = final_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "statePop = pd.read_csv(\"StatePop.csv\") # state populations\n",
    "# turn state name into acronym\n",
    "statePop['State'] = statePop.apply(lambda x: states_hash[x['State']], axis=1)\n",
    "# add population column to final_df\n",
    "final_df = pd.merge(final_df, statePop, how='inner', on=['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of feature column names 'positivityRate',\n",
    "features = list(final_df.columns)\n",
    "for col in ['State', 'Period', 'Depression_Score', 'Anxiety_Score', 'Mix_Score', ]:\n",
    "    features.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features by population, except positivtyRate\n",
    "for feature in features:\n",
    "    final_df.loc[:,feature] = final_df.loc[:,feature]/final_df['Pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop redundant columns\n",
    "final_df = final_df.drop(columns=[\"Pop\", \"density\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"Clean_Data/final_data.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Period</th>\n",
       "      <th>Mix_Score</th>\n",
       "      <th>Depression_Score</th>\n",
       "      <th>Anxiety_Score</th>\n",
       "      <th>deathIncrease</th>\n",
       "      <th>death</th>\n",
       "      <th>hospitalizedCurrently</th>\n",
       "      <th>positivityRate</th>\n",
       "      <th>positiveTestsViralIncrease</th>\n",
       "      <th>...</th>\n",
       "      <th>Number of Trips &lt;1</th>\n",
       "      <th>Number of Trips 1-3</th>\n",
       "      <th>Number of Trips 3-5</th>\n",
       "      <th>Number of Trips 5-10</th>\n",
       "      <th>Number of Trips 10-25</th>\n",
       "      <th>Number of Trips 25-50</th>\n",
       "      <th>Number of Trips 50-100</th>\n",
       "      <th>Number of Trips 100-250</th>\n",
       "      <th>Number of Trips 250-500</th>\n",
       "      <th>Number of Trips &gt;=500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "      <td>30.3</td>\n",
       "      <td>18.6</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.363889</td>\n",
       "      <td>20.645671</td>\n",
       "      <td>11.001685</td>\n",
       "      <td>13.193620</td>\n",
       "      <td>12.409914</td>\n",
       "      <td>4.195485</td>\n",
       "      <td>1.461642</td>\n",
       "      <td>0.537095</td>\n",
       "      <td>0.111032</td>\n",
       "      <td>0.020167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>2</td>\n",
       "      <td>30.6</td>\n",
       "      <td>22.5</td>\n",
       "      <td>27.2</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.633588</td>\n",
       "      <td>10.701244</td>\n",
       "      <td>5.832369</td>\n",
       "      <td>7.063924</td>\n",
       "      <td>6.676520</td>\n",
       "      <td>2.289233</td>\n",
       "      <td>0.827562</td>\n",
       "      <td>0.311962</td>\n",
       "      <td>0.069180</td>\n",
       "      <td>0.010925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>3</td>\n",
       "      <td>25.2</td>\n",
       "      <td>19.6</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.222031</td>\n",
       "      <td>11.008505</td>\n",
       "      <td>5.970319</td>\n",
       "      <td>7.238895</td>\n",
       "      <td>6.895172</td>\n",
       "      <td>2.429710</td>\n",
       "      <td>0.879017</td>\n",
       "      <td>0.338662</td>\n",
       "      <td>0.079815</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>4</td>\n",
       "      <td>28.8</td>\n",
       "      <td>20.9</td>\n",
       "      <td>25.2</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.582299</td>\n",
       "      <td>10.575117</td>\n",
       "      <td>5.755132</td>\n",
       "      <td>7.097441</td>\n",
       "      <td>6.701813</td>\n",
       "      <td>2.295237</td>\n",
       "      <td>0.882371</td>\n",
       "      <td>0.396369</td>\n",
       "      <td>0.097929</td>\n",
       "      <td>0.018569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>5</td>\n",
       "      <td>37.5</td>\n",
       "      <td>28.4</td>\n",
       "      <td>31.6</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.757588</td>\n",
       "      <td>10.538668</td>\n",
       "      <td>5.664647</td>\n",
       "      <td>6.900937</td>\n",
       "      <td>6.577336</td>\n",
       "      <td>2.276705</td>\n",
       "      <td>0.864323</td>\n",
       "      <td>0.405511</td>\n",
       "      <td>0.085880</td>\n",
       "      <td>0.011446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>KS</td>\n",
       "      <td>14</td>\n",
       "      <td>32.5</td>\n",
       "      <td>19.3</td>\n",
       "      <td>28.7</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>21.054919</td>\n",
       "      <td>22.569022</td>\n",
       "      <td>9.641422</td>\n",
       "      <td>11.786638</td>\n",
       "      <td>12.316341</td>\n",
       "      <td>5.014209</td>\n",
       "      <td>2.146498</td>\n",
       "      <td>1.231252</td>\n",
       "      <td>0.294709</td>\n",
       "      <td>0.130185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>KS</td>\n",
       "      <td>15</td>\n",
       "      <td>35.2</td>\n",
       "      <td>22.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.674147</td>\n",
       "      <td>22.558298</td>\n",
       "      <td>9.746595</td>\n",
       "      <td>11.716151</td>\n",
       "      <td>12.151250</td>\n",
       "      <td>4.928880</td>\n",
       "      <td>2.111289</td>\n",
       "      <td>1.027574</td>\n",
       "      <td>0.212464</td>\n",
       "      <td>0.053351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>KS</td>\n",
       "      <td>16</td>\n",
       "      <td>36.2</td>\n",
       "      <td>22.7</td>\n",
       "      <td>32.3</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.287248</td>\n",
       "      <td>21.236292</td>\n",
       "      <td>9.165584</td>\n",
       "      <td>11.180588</td>\n",
       "      <td>11.768170</td>\n",
       "      <td>4.839855</td>\n",
       "      <td>2.191240</td>\n",
       "      <td>1.056929</td>\n",
       "      <td>0.217324</td>\n",
       "      <td>0.044964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>KS</td>\n",
       "      <td>17</td>\n",
       "      <td>39.6</td>\n",
       "      <td>26.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.284037</td>\n",
       "      <td>23.066331</td>\n",
       "      <td>9.835879</td>\n",
       "      <td>11.610815</td>\n",
       "      <td>11.836634</td>\n",
       "      <td>4.795822</td>\n",
       "      <td>2.141347</td>\n",
       "      <td>1.034027</td>\n",
       "      <td>0.186494</td>\n",
       "      <td>0.046568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>KS</td>\n",
       "      <td>18</td>\n",
       "      <td>43.6</td>\n",
       "      <td>30.9</td>\n",
       "      <td>40.4</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.167183</td>\n",
       "      <td>8.022979</td>\n",
       "      <td>3.524033</td>\n",
       "      <td>4.177374</td>\n",
       "      <td>4.233812</td>\n",
       "      <td>1.657593</td>\n",
       "      <td>0.716925</td>\n",
       "      <td>0.416102</td>\n",
       "      <td>0.076721</td>\n",
       "      <td>0.018665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>853 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    State  Period  Mix_Score  Depression_Score  Anxiety_Score  deathIncrease  \\\n",
       "0      AL       1       30.3              18.6           25.6       0.000002   \n",
       "1      AL       2       30.6              22.5           27.2       0.000003   \n",
       "2      AL       3       25.2              19.6           20.7       0.000002   \n",
       "3      AL       4       28.8              20.9           25.2       0.000002   \n",
       "4      AL       5       37.5              28.4           31.6       0.000002   \n",
       "..    ...     ...        ...               ...            ...            ...   \n",
       "848    KS      14       32.5              19.3           28.7       0.000002   \n",
       "849    KS      15       35.2              22.9           30.6       0.000003   \n",
       "850    KS      16       36.2              22.7           32.3       0.000004   \n",
       "851    KS      17       39.6              26.2           34.8       0.000005   \n",
       "852    KS      18       43.6              30.9           40.4       0.000005   \n",
       "\n",
       "        death  hospitalizedCurrently  positivityRate  \\\n",
       "0    0.000051               0.000092             0.0   \n",
       "1    0.000079               0.000096             0.0   \n",
       "2    0.000099               0.000101             0.0   \n",
       "3    0.000112               0.000104             0.0   \n",
       "4    0.000127               0.000115             0.0   \n",
       "..        ...                    ...             ...   \n",
       "848  0.000169               0.000085             0.0   \n",
       "849  0.000210               0.000100             0.0   \n",
       "850  0.000248               0.000131             0.0   \n",
       "851  0.000312               0.000136             0.0   \n",
       "852  0.000371               0.000181             0.0   \n",
       "\n",
       "     positiveTestsViralIncrease  ...  Number of Trips <1  Number of Trips 1-3  \\\n",
       "0                           NaN  ...           18.363889            20.645671   \n",
       "1                           NaN  ...            9.633588            10.701244   \n",
       "2                           NaN  ...           10.222031            11.008505   \n",
       "3                           NaN  ...            9.582299            10.575117   \n",
       "4                           NaN  ...            9.757588            10.538668   \n",
       "..                          ...  ...                 ...                  ...   \n",
       "848                         NaN  ...           21.054919            22.569022   \n",
       "849                         NaN  ...           20.674147            22.558298   \n",
       "850                         NaN  ...           19.287248            21.236292   \n",
       "851                         NaN  ...           20.284037            23.066331   \n",
       "852                         NaN  ...            7.167183             8.022979   \n",
       "\n",
       "     Number of Trips 3-5  Number of Trips 5-10  Number of Trips 10-25  \\\n",
       "0              11.001685             13.193620              12.409914   \n",
       "1               5.832369              7.063924               6.676520   \n",
       "2               5.970319              7.238895               6.895172   \n",
       "3               5.755132              7.097441               6.701813   \n",
       "4               5.664647              6.900937               6.577336   \n",
       "..                   ...                   ...                    ...   \n",
       "848             9.641422             11.786638              12.316341   \n",
       "849             9.746595             11.716151              12.151250   \n",
       "850             9.165584             11.180588              11.768170   \n",
       "851             9.835879             11.610815              11.836634   \n",
       "852             3.524033              4.177374               4.233812   \n",
       "\n",
       "     Number of Trips 25-50  Number of Trips 50-100  Number of Trips 100-250  \\\n",
       "0                 4.195485                1.461642                 0.537095   \n",
       "1                 2.289233                0.827562                 0.311962   \n",
       "2                 2.429710                0.879017                 0.338662   \n",
       "3                 2.295237                0.882371                 0.396369   \n",
       "4                 2.276705                0.864323                 0.405511   \n",
       "..                     ...                     ...                      ...   \n",
       "848               5.014209                2.146498                 1.231252   \n",
       "849               4.928880                2.111289                 1.027574   \n",
       "850               4.839855                2.191240                 1.056929   \n",
       "851               4.795822                2.141347                 1.034027   \n",
       "852               1.657593                0.716925                 0.416102   \n",
       "\n",
       "     Number of Trips 250-500  Number of Trips >=500  \n",
       "0                   0.111032               0.020167  \n",
       "1                   0.069180               0.010925  \n",
       "2                   0.079815               0.011400  \n",
       "3                   0.097929               0.018569  \n",
       "4                   0.085880               0.011446  \n",
       "..                       ...                    ...  \n",
       "848                 0.294709               0.130185  \n",
       "849                 0.212464               0.053351  \n",
       "850                 0.217324               0.044964  \n",
       "851                 0.186494               0.046568  \n",
       "852                 0.076721               0.018665  \n",
       "\n",
       "[853 rows x 26 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
